{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "import pyro\n",
    "import torch\n",
    "import pyro.distributions as dist\n",
    "\n",
    "from vectorized_loop.ops import Index\n",
    "import vectorized_loop as vec\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enumeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:vectorized_loop.vectorized_loop_messenger:a (vectorized): repeat 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'type': 'sample',\n",
       " 'name': 'x',\n",
       " 'fn': BranchDistribution(site_shape=(10,), event_shape=torch.Size([]), conds=[tensor(True)], dists=[Categorical(probs: torch.Size([3, 1, 10, 3]))]),\n",
       " 'is_observed': False,\n",
       " 'args': (),\n",
       " 'kwargs': {},\n",
       " 'value': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]]),\n",
       " 'infer': {'enumerate': 'parallel', '_enumerate_dim': -2},\n",
       " 'scale': 1.0,\n",
       " 'mask': None,\n",
       " 'cond_indep_stack': (CondIndepStackFrame(name='a', dim=-1, size=10, counter=None, full_size=10),),\n",
       " 'done': True,\n",
       " 'stop': False,\n",
       " 'continuation': None}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@vec.enum\n",
    "@vec.vectorize\n",
    "def model(s: vec.State):\n",
    "    s.z = torch.tensor([1, 2, 3])  # (, | 3)\n",
    "    s.x = 0  # ()\n",
    "    for i in vec.range(\"a\", 10, vectorized=True):\n",
    "        s.y = s.z + s.x.unsqueeze(-1)  # (10 | 3) -> (3, 1 | 10 | 3)\n",
    "        s.x = pyro.sample(\n",
    "            \"x\",\n",
    "            dist.Categorical(s.y),\n",
    "            infer={\"enumerate\": \"parallel\"},\n",
    "        )  # (3 | 10 | ,)\n",
    "\n",
    "vec.clear_allocators()\n",
    "vec.trace(model).get_trace().nodes[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:vectorized_loop.vectorized_loop_messenger:b (vectorized): repeat 2\n",
      "DEBUG:vectorized_loop.vectorized_loop_messenger:b (vectorized): repeat 2\n",
      "DEBUG:vectorized_loop.vectorized_loop_messenger:a (vectorized): repeat 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5, 1, 5, 1, 10])\n",
      "True\n",
      "torch.Size([5, 5, 5, 10, 10])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "@vec.vectorize\n",
    "def model(s: vec.State, vectorized, P):\n",
    "    s.x = 2\n",
    "    s.y = 1\n",
    "    for i in vec.range(\"a\", 10, vectorized=vectorized):  # (a: -1)\n",
    "        s.x = pyro.sample(\"x\", dist.Categorical(P[(s.x.long() * s.y.long()) % 5]), infer={\"enumerate\": \"parallel\"})  # (x_curr: -3 -> x_prev: -6)\n",
    "        for j in vec.range(\"b\", 10, vectorized=vectorized):  # (b: -2)\n",
    "            s.y = pyro.sample(\"y\", dist.Categorical(P[(s.x.long() + s.y.long()) % 5]), infer={\"enumerate\": \"parallel\"})  # (y_curr: -4 -> y_prev: -5)\n",
    "\n",
    "P = torch.rand(5, 5)\n",
    "P = P / P.sum(dim=1, keepdim=True)\n",
    "\n",
    "vec.clear_allocators()\n",
    "tr = vec.trace(vec.enum(model)).get_trace(True, P)\n",
    "tr.compute_log_prob()\n",
    "x_log_prob_1 = tr.nodes[\"x\"][\"log_prob\"]\n",
    "y_log_prob_1 = tr.nodes[\"y\"][\"log_prob\"]\n",
    "\n",
    "vec.clear_allocators()\n",
    "tr = vec.trace(vec.enum(model)).get_trace(False, P)\n",
    "tr.compute_log_prob()\n",
    "x_log_prob_2 = tr.nodes[\"x\"][\"log_prob\"]\n",
    "y_log_prob_2 = tr.nodes[\"y\"][\"log_prob\"]\n",
    "\n",
    "print(x_log_prob_1.shape)  # (x_prev, y_prev, -,      x_curr | -, a)\n",
    "print(torch.allclose(x_log_prob_1, x_log_prob_2))\n",
    "print(y_log_prob_1.shape)  # (-     , y_prev, y_curr, x_curr | b, a)\n",
    "print(torch.allclose(y_log_prob_1, y_log_prob_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:vectorized_loop.vectorized_loop_messenger:a (vectorized): repeat 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n",
      "torch.Size([5, 1, 5, 10])\n",
      "torch.Size([5, 1, 10])\n",
      "torch.Size([5, 1, 5, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "@vec.vectorize\n",
    "def model(s: vec.State, vectorized):\n",
    "    P = torch.ones(5, 5) / 5\n",
    "    s.x = 0\n",
    "    s.y = 0\n",
    "    for i in vec.range(\"a\", 10, vectorized=vectorized):  # (a: -1)\n",
    "        s.x = pyro.sample(\"x\", dist.Categorical(P[s.x.long()]), infer={\"enumerate\": \"parallel\"})  # (x_curr: -2 -> x_prev: -4)\n",
    "        s.y = pyro.sample(\"y\", dist.Categorical(P[s.y.long()]), infer={\"enumerate\": \"parallel\"})  # (y_curr: -3 -> y_prev: -5)\n",
    "\n",
    "vec.clear_allocators()\n",
    "tr = vec.trace(vec.enum(model)).get_trace(True)\n",
    "print(tr.nodes[\"x\"][\"value\"].shape)                                # (-     , -     , -     , x_curr | a)\n",
    "print(tr.nodes[\"x\"][\"fn\"].log_prob(tr.nodes[\"x\"][\"value\"]).shape)  # (-     , x_prev, -     , x_curr | a)\n",
    "print(tr.nodes[\"y\"][\"value\"].shape)                                # (-     , -     , y_curr, -      | a)\n",
    "print(tr.nodes[\"y\"][\"fn\"].log_prob(tr.nodes[\"y\"][\"value\"]).shape)  # (y_prev, -     , y_curr, -      | a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:vectorized_loop.vectorized_loop_messenger:b (vectorized): repeat 1\n",
      "DEBUG:vectorized_loop.vectorized_loop_messenger:b (vectorized): repeat 1\n",
      "DEBUG:vectorized_loop.vectorized_loop_messenger:a (vectorized): repeat 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 3])\n",
      "torch.Size([5, 1, 5, 1, 3])\n",
      "torch.Size([5, 1, 3, 3])\n",
      "torch.Size([5, 5, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "@vec.vectorize\n",
    "def model(s: vec.State):\n",
    "    P = torch.rand(5, 5).log()\n",
    "    s.x = 2\n",
    "    for _ in vec.range(\"a\", 3, vectorized=True):\n",
    "        s.x = pyro.sample(\"x\", dist.Categorical(logits=Index(P)[s.x]), infer={\"enumerate\": \"parallel\"})  # (x_curr: -3 -> x_prev: -5)\n",
    "        s.z = s.x  # (x_curr | -, a)\n",
    "        for _ in vec.range(\"b\", 3, vectorized=True):\n",
    "            s.y = pyro.sample(\"y\", dist.Categorical(logits=Index(P)[s.x]), infer={\"enumerate\": \"parallel\"})  # (y_curr: -4)\n",
    "            s.z = s.x + s.y  # (y_curr, x_curr | b, a)\n",
    "        \n",
    "vec.clear_allocators()\n",
    "\n",
    "tr_model = vec.trace(vec.enum(model)).get_trace()\n",
    "tr_model.compute_log_prob()\n",
    "\n",
    "print(tr_model.nodes[\"x\"][\"value\"].shape)     # (-     , -     , x_curr | - , a)\n",
    "print(tr_model.nodes[\"x\"][\"log_prob\"].shape)  # (x_prev, -     , x_curr | - , a)\n",
    "print(tr_model.nodes[\"y\"][\"value\"].shape)     # (-     , y_curr, -      | b , a)\n",
    "print(tr_model.nodes[\"y\"][\"log_prob\"].shape)  # (-     , y_curr, x_curr | b , a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enumeration with branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:vectorized_loop.vectorized_loop_messenger:a (vectorized): repeat 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5, 3])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "@vec.vectorize\n",
    "def model(s: vec.State, P, is_guide, vectorized):\n",
    "    if not is_guide:\n",
    "        s.x = 2\n",
    "        for s.i in vec.range(\"a\", 3, vectorized=vectorized):\n",
    "            s.y = s.x\n",
    "            with vec.branch(s.i >= 1):\n",
    "                s.x = pyro.sample(\"x\", dist.Categorical(logits=Index(P)[s.y]), infer={\"enumerate\": \"parallel\"})\n",
    "            with vec.branch(s.i == 0):\n",
    "                s.x = pyro.sample(\"x\", dist.Categorical(logits=Index(P)[(s.y + 1) % 5]), infer={\"enumerate\": \"parallel\"})\n",
    "\n",
    "P = torch.rand(5, 5).log()\n",
    "P = P - P.logsumexp(-1, True)\n",
    "\n",
    "vec.clear_allocators()\n",
    "tr_guide = vec.trace(model).get_trace(P, is_guide=True, vectorized=False)\n",
    "tr_model = vec.trace(vec.replay(vec.enum(model), tr_guide)).get_trace(P, is_guide=False, vectorized=False)\n",
    "tr_model.compute_log_prob()\n",
    "log_prob_1 = tr_model.nodes[\"x\"][\"log_prob\"]\n",
    "\n",
    "vec.clear_allocators()\n",
    "tr_guide = vec.trace(model).get_trace(P, is_guide=True, vectorized=False)\n",
    "tr_model = vec.trace(vec.replay(vec.enum(model), tr_guide)).get_trace(P, is_guide=False, vectorized=True)\n",
    "tr_model.compute_log_prob()\n",
    "log_prob_2 = tr_model.nodes[\"x\"][\"log_prob\"]\n",
    "\n",
    "print(log_prob_1.shape)\n",
    "print(torch.allclose(log_prob_1, log_prob_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic plates and markovs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, vectorized):\n",
    "    tr_guide = vec.trace(model).get_trace(False)\n",
    "    tr_model = vec.trace(vec.replay(model, tr_guide)).get_trace(vectorized)\n",
    "\n",
    "    for name, site in tr_guide.nodes.items():\n",
    "        if site[\"type\"] == \"sample\":\n",
    "            print(\"Guide: %s ~ %s\" % (site[\"value\"], site[\"fn\"]))\n",
    "\n",
    "    for name, site in tr_model.nodes.items():\n",
    "        if site[\"type\"] == \"sample\":\n",
    "            print(\"Model: %s ~ %s\" % (site[\"value\"], site[\"fn\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guide: tensor([-0.9255,  0.7853,  0.7795]) ~ BranchDistribution(site_shape=(3,), event_shape=torch.Size([]), conds=[tensor([ True, False, False]), tensor([False,  True, False]), tensor([False, False,  True])], dists=[Normal(loc: 0.0, scale: 1.0), Normal(loc: 0.0, scale: 1.0), Normal(loc: 0.0, scale: 1.0)])\n",
      "Guide: tensor([-0.1979,  0.3649,  0.4731]) ~ BranchDistribution(site_shape=(3,), event_shape=torch.Size([]), conds=[tensor([ True, False, False]), tensor([False,  True, False]), tensor([False, False,  True])], dists=[Normal(loc: -0.9254725575447083, scale: 1.0), Normal(loc: 0.7852697372436523, scale: 1.0), Normal(loc: 0.7795404195785522, scale: 1.0)])\n",
      "Model: tensor([-0.9255,  0.7853,  0.7795]) ~ BranchDistribution(site_shape=(3,), event_shape=torch.Size([]), conds=[tensor([ True, False, False]), tensor([False,  True, False]), tensor([False, False,  True])], dists=[Normal(loc: 0.0, scale: 1.0), Normal(loc: 0.0, scale: 1.0), Normal(loc: 0.0, scale: 1.0)])\n",
      "Model: tensor([-0.1979,  0.3649,  0.4731]) ~ BranchDistribution(site_shape=(3,), event_shape=torch.Size([]), conds=[tensor([ True, False, False]), tensor([False,  True, False]), tensor([False, False,  True])], dists=[Normal(loc: -0.9254725575447083, scale: 1.0), Normal(loc: 0.7852697372436523, scale: 1.0), Normal(loc: 0.7795404195785522, scale: 1.0)])\n"
     ]
    }
   ],
   "source": [
    "# 1-level nested plates\n",
    "\n",
    "@vec.vectorize\n",
    "def model(s: vec.State, vectorized):\n",
    "    for i in vec.range(\"a\", size=3, vectorized=vectorized):\n",
    "        s.x = pyro.sample(\"x\", dist.Normal(0, 1))\n",
    "        s.y = pyro.sample(\"y\", dist.Normal(s.x, 1))\n",
    "\n",
    "vec.clear_allocators()\n",
    "# run(model, True)\n",
    "run(model, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:vectorized_loop.vectorized_loop_messenger:b (vectorized): repeat 1\n",
      "DEBUG:vectorized_loop.vectorized_loop_messenger:a (vectorized): repeat 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guide: tensor([[1.0888, 0.9823],\n",
      "        [0.6738, 1.1079],\n",
      "        [1.1436, 0.3730]]) ~ BranchDistribution(site_shape=(3, 2), event_shape=torch.Size([]), conds=[tensor([[ True, False],\n",
      "        [False, False],\n",
      "        [False, False]]), tensor([[False, False],\n",
      "        [ True, False],\n",
      "        [False, False]]), tensor([[False, False],\n",
      "        [False, False],\n",
      "        [ True, False]]), tensor([[False,  True],\n",
      "        [False, False],\n",
      "        [False, False]]), tensor([[False, False],\n",
      "        [False,  True],\n",
      "        [False, False]]), tensor([[False, False],\n",
      "        [False, False],\n",
      "        [False,  True]])], dists=[Normal(loc: 0.0, scale: 1.0), Normal(loc: 0.0, scale: 1.0), Normal(loc: 0.0, scale: 1.0), Normal(loc: 0.0, scale: 1.0), Normal(loc: 0.0, scale: 1.0), Normal(loc: 0.0, scale: 1.0)])\n",
      "Guide: tensor([[ 2.1782,  0.6153],\n",
      "        [ 1.0825,  0.0347],\n",
      "        [ 1.7723, -0.9003]]) ~ BranchDistribution(site_shape=(3, 2), event_shape=torch.Size([]), conds=[tensor([[ True, False],\n",
      "        [False, False],\n",
      "        [False, False]]), tensor([[False, False],\n",
      "        [ True, False],\n",
      "        [False, False]]), tensor([[False, False],\n",
      "        [False, False],\n",
      "        [ True, False]]), tensor([[False,  True],\n",
      "        [False, False],\n",
      "        [False, False]]), tensor([[False, False],\n",
      "        [False,  True],\n",
      "        [False, False]]), tensor([[False, False],\n",
      "        [False, False],\n",
      "        [False,  True]])], dists=[Normal(loc: 1.088820219039917, scale: 1.0), Normal(loc: 0.6737640500068665, scale: 1.0), Normal(loc: 1.1436402797698975, scale: 1.0), Normal(loc: 0.982276976108551, scale: 1.0), Normal(loc: 1.1079471111297607, scale: 1.0), Normal(loc: 0.37304922938346863, scale: 1.0)])\n",
      "Model: tensor([[1.0888, 0.9823],\n",
      "        [0.6738, 1.1079],\n",
      "        [1.1436, 0.3730]]) ~ BranchDistribution(site_shape=(3, 2), event_shape=torch.Size([]), conds=[tensor(True)], dists=[Normal(loc: torch.Size([3, 2]), scale: torch.Size([3, 2]))])\n",
      "Model: tensor([[ 2.1782,  0.6153],\n",
      "        [ 1.0825,  0.0347],\n",
      "        [ 1.7723, -0.9003]]) ~ BranchDistribution(site_shape=(3, 2), event_shape=torch.Size([]), conds=[tensor(True)], dists=[Normal(loc: torch.Size([3, 2]), scale: torch.Size([3, 2]))])\n"
     ]
    }
   ],
   "source": [
    "# 2-level nested plates\n",
    "\n",
    "@vec.vectorize\n",
    "def model(s: vec.State, vectorized):\n",
    "    for _ in vec.range(\"a\", size=2, vectorized=vectorized):\n",
    "        for _ in vec.range(\"b\", size=3, vectorized=vectorized):\n",
    "            s.x = pyro.sample(\"x\", dist.Normal(0, 1))\n",
    "            s.y = pyro.sample(\"y\", dist.Normal(s.x, 1))\n",
    "\n",
    "vec.clear_allocators()\n",
    "run(model, True)\n",
    "# run(model, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:vectorized_loop.vectorized_loop_messenger:a (vectorized): repeat 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guide: tensor([10.5045, 10.1355,  9.3305]) ~ BranchDistribution(site_shape=(3,), event_shape=torch.Size([]), conds=[tensor([ True, False, False]), tensor([False,  True, False]), tensor([False, False,  True])], dists=[Normal(loc: 10.0, scale: 1.0), Normal(loc: 10.504530906677246, scale: 1.0), Normal(loc: 10.135538101196289, scale: 1.0)])\n",
      "Model: tensor([10.5045, 10.1355,  9.3305]) ~ BranchDistribution(site_shape=(3,), event_shape=torch.Size([]), conds=[tensor(True)], dists=[Normal(loc: torch.Size([3]), scale: torch.Size([3]))])\n"
     ]
    }
   ],
   "source": [
    "# 1 level nested markovs\n",
    "\n",
    "@vec.vectorize\n",
    "def model(s: vec.State, vectorized):\n",
    "    s.x = 10\n",
    "    for _ in vec.range(\"a\", size=3, vectorized=vectorized):\n",
    "        s.x = pyro.sample(\"x\", dist.Normal(s.x, 1))\n",
    "        \n",
    "vec.clear_allocators()\n",
    "run(model, True)\n",
    "# run(model, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:vectorized_loop.vectorized_loop_messenger:b (vectorized): repeat 2\n",
      "DEBUG:vectorized_loop.vectorized_loop_messenger:b (vectorized): repeat 2\n",
      "DEBUG:vectorized_loop.vectorized_loop_messenger:a (vectorized): repeat 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guide: tensor([[9.4325, 8.9606, 8.8674],\n",
      "        [8.0014, 7.1650, 6.5959]]) ~ BranchDistribution(site_shape=(2, 3), event_shape=torch.Size([]), conds=[tensor([[ True, False, False],\n",
      "        [False, False, False]]), tensor([[False,  True, False],\n",
      "        [False, False, False]]), tensor([[False, False,  True],\n",
      "        [False, False, False]]), tensor([[False, False, False],\n",
      "        [ True, False, False]]), tensor([[False, False, False],\n",
      "        [False,  True, False]]), tensor([[False, False, False],\n",
      "        [False, False,  True]])], dists=[Normal(loc: 10.0, scale: 1.0), Normal(loc: 9.432474136352539, scale: 1.0), Normal(loc: 8.960566520690918, scale: 1.0), Normal(loc: 8.867393493652344, scale: 1.0), Normal(loc: 8.001392364501953, scale: 1.0), Normal(loc: 7.165006637573242, scale: 1.0)])\n",
      "Model: tensor([[9.4325, 8.9606, 8.8674],\n",
      "        [8.0014, 7.1650, 6.5959]]) ~ BranchDistribution(site_shape=(2, 3), event_shape=torch.Size([]), conds=[tensor(True)], dists=[Normal(loc: torch.Size([2, 3]), scale: torch.Size([2, 3]))])\n"
     ]
    }
   ],
   "source": [
    "# 2 level nested markovs\n",
    "\n",
    "@vec.vectorize\n",
    "def model(s: vec.State, vectorized):\n",
    "    s.x = 10\n",
    "    for _ in vec.range(\"a\", size=2, dim=-2, vectorized=vectorized):\n",
    "        for _ in vec.range(\"b\", size=3, dim=-1, vectorized=vectorized):\n",
    "            s.x = pyro.sample(\"x\", dist.Normal(s.x, 1))\n",
    "\n",
    "vec.clear_allocators()\n",
    "run(model, True)\n",
    "# run(model, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
